{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26ab996",
   "metadata": {},
   "source": [
    "# Understand the kernel with code samples\n",
    "\n",
    "----\n",
    "The kernel is the central component of Semantic Kernel. At its simplest, the kernel is a Dependency Injection container that manages all of the services and plugins necessary to run your AI application. If you provide all of your services and plugins to the kernel, they will then be seamlessly used by the AI as needed. \n",
    "\n",
    "| Components | Description |\n",
    "|------------|-------------|\n",
    "| **1. Services** | These consist of both AI services (e.g., chat completion) and other services (e.g., logging and HTTP clients) that are necessary to run your application. This was modeled after the Service Provider pattern in .NET to support dependency injection across all languages. |\n",
    "| **2. Plugins** | These are components used by your AI services and prompt templates to perform work. For example, AI services can use plugins to retrieve data from a database or call an external API to perform actions. |\n",
    "\n",
    "![kernel](images/the-kernel-is-at-the-center-of-everything.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e456fe",
   "metadata": {},
   "source": [
    "## Hierarchical Architecture Diagram\n",
    "---\n",
    "\n",
    "Architecture of a Retrieval-Augmented Generation (RAG) based chatbot using the latest Semantic Kernel. The diagram highlights how the Agent Framework (for chat interactions) and Process Framework (for workflow orchestration) sit above the core Kernel, leveraging it to perform AI tasks. The Semantic Kernel core orchestrates between user requests and the underlying AI services, memory stores, and plugins. It integrates with external LLM services for generation, uses a vector DB for semantic memory (retrieval of relevant data), and can invoke plugins which may call other external APIs or services as needed.\n",
    "\n",
    "![Hierarchical](images/semantic_kernel_component.png)\n",
    "\n",
    "***AI Agent (Agent Framework)***: The Agent Framework in Semantic Kernel is an optional layer that helps create conversational AI agents (like chatbots) using the core kernel’s capabilities​. It is not a replacement for the kernel but builds on top of it – your application still includes the Semantic Kernel library, and the agent uses the kernel’s functions internally. In a chatbot scenario, the Agent Framework manages the dialogue (turn-taking, system prompts, etc.) while delegating AI tasks to the kernel.\n",
    "\n",
    "***Semantic Kernel (Core)***: The Semantic Kernel core is the heart of the system. It orchestrates calls to AI models, retrieves memories, and executes plugin functions. The kernel provides abstractions to connect to AI services (LLM APIs for text generation, embedding models, etc.) and to various memory stores (for example, vector databases)​. It also manages context (prompts) and can use Planners to chain or select functions dynamically to fulfill a user request​. The kernel itself is part of your app’s runtime, coordinating all other components.\n",
    "\n",
    "***Plugins (Skills/Functions)***: Plugins (also called skills or functions) are units of functionality that the kernel can invoke. They might be defined with natural language prompts (semantic functions) or as native code functions. Plugins can perform calculations, transform data, or call external services/APIs. They are registered with and executed via the kernel, meaning they depend on the kernel to be invoked as part of an AI workflow​. However, the plugin implementations (e.g. an HTTP call to a web service, a database query) run outside the kernel – the kernel just orchestrates their usage. In essence, plugins extend the kernel’s abilities, and the kernel can automatically chain plugins to accomplish complex tasks for the user​.\n",
    "\n",
    "***AI Services***: These are external AI model endpoints that the kernel calls through its connectors. For example, the OpenAI or Azure OpenAI service provides the GPT-4 model for text generation, and there are embedding model services for vector generation. The kernel has integrations for many AI services (text completion, chat, image generation, speech recognition, etc.) and it uses them by calling out to the respective APIs​. These services are not “inside” the kernel – instead, the kernel depends on them to provide the intelligence. In the architecture, they appear as external components that the kernel invokes (e.g. sending a prompt to the GPT model and getting a completion).\n",
    "\n",
    "***Semantic Memory (Vector DB)***: The Semantic Kernel includes a memory abstraction that allows storing and retrieving contextual information. Under the hood, this is often backed by a vector database or search index. In a RAG-based chatbot, this is critical: documents or knowledge are embedded into vector representations and stored, so that relevant pieces can be retrieved to ground the AI’s answers. Semantic Kernel’s memory can integrate with many vector stores (e.g. Azure Cognitive Search, ChromaDB, Qdrant, Redis, Pinecone, etc.)​. This means “Memory” is essentially an AI Search over a vector DB, enabling the bot to find relevant information by semantic similarity. The memory component is used by the kernel but the database itself is external – the kernel just sends queries and gets results.\n",
    "\n",
    "***Process Framework (Workflow Orchestration)***: The Process Framework is another optional layer in the latest Semantic Kernel, aimed at long-running or multi-step workflows. It lets developers define structured processes composed of multiple steps, where each step can call kernel functions (AI or non-AI tasks) in an event-driven sequence​. Like the Agent Framework, the Process Framework builds on the kernel rather than enclosing it. It uses the kernel to execute AI functions at each step of a business workflow. This is especially useful if your chatbot or assistant needs to carry out complex transactions or procedures (for example, an order processing workflow or a support ticket resolution that involves several back-and-forth steps) beyond a single conversational turn. The Process Framework uses technologies like Orleans or Dapr under the hood for reliability and can reuse any existing kernel plugins within those processes​."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b243d6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1d038",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "651e17cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.agents import AssistantAgentThread, AzureAssistantAgent\n",
    "from semantic_kernel.kernel_pydantic import KernelBaseSettings\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "from semantic_kernel.connectors.ai import FunctionChoiceBehavior\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.filters import FilterTypes, FunctionInvocationContext\n",
    "from collections.abc import Awaitable, Callable\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.agents import AzureAIAgent, AzureAIAgentSettings, AzureAIAgentThread\n",
    "from semantic_kernel.agents import AzureResponsesAgent, ResponsesAgentThread\n",
    "from semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.agents import AzureResponsesAgent\n",
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from semantic_kernel.agents.strategies.termination.termination_strategy import TerminationStrategy\n",
    "from semantic_kernel.agents import AgentGroupChat\n",
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from azure.ai.projects.models import OpenApiAnonymousAuthDetails, OpenApiTool, CodeInterpreterTool\n",
    "from semantic_kernel.contents import AuthorRole\n",
    "from semantic_kernel.contents.streaming_chat_message_content import StreamingChatMessageContent\n",
    "\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "\n",
    "\n",
    "class Service(Enum):\n",
    "    \"\"\"Attributes:\n",
    "    OpenAI (str): Represents the OpenAI service.\n",
    "    AzureOpenAI (str): Represents the Azure OpenAI service.\n",
    "    HuggingFace (str): Represents the HuggingFace service.\n",
    "    \"\"\"\n",
    "\n",
    "    OpenAI = \"openai\"\n",
    "    AzureOpenAI = \"azureopenai\"\n",
    "    HuggingFace = \"huggingface\"\n",
    "\n",
    "class ServiceSettings(KernelBaseSettings):\n",
    "    \"\"\"The Learn Resources Service Settings.\n",
    "\n",
    "    The settings are first loaded from environment variables. If the\n",
    "    environment variables are not found, the settings can be loaded from a .env file with the\n",
    "    encoding 'utf-8' as default or the specific encoding. If the settings are not found in the\n",
    "    .env file, the settings are ignored; however, validation will fail alerting that the settings\n",
    "    are missing.\n",
    "\n",
    "    Args:\n",
    "        global_llm_service (str | None): The LLM service to use for the samples, either \"OpenAI\" or \"AzureOpenAI\"\n",
    "            If not provided, defaults to \"AzureOpenAI\".\n",
    "    \"\"\"\n",
    "\n",
    "    global_llm_service: str | None = None\n",
    "    \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b01f0",
   "metadata": {},
   "source": [
    "We will load our settings and get the LLM service to use for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b0c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "service_settings = ServiceSettings.create()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ed61f",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ed56686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c79873",
   "metadata": {},
   "source": [
    "# Run a Semantic Function (No Agent, No Process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd6ea0",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.1 running a prompt without plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a84f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:> "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft was founded by Bill Gates and Paul Allen in 1975,  \n",
      "Initially developed a version of BASIC for the Altair 8800,  \n",
      "In 1980, Microsoft signed a contract with IBM to provide an operating system,  \n",
      "This led to the creation of MS-DOS, which became the industry standard,  \n",
      "The introduction of Windows in 1985 marked a significant shift towards graphical user interfaces,  \n",
      "By the 1990s, Windows became the dominant OS for personal computers,  \n",
      "Microsoft Office suite was launched, enhancing productivity for businesses,  \n",
      "The company went public in 1986, making Gates one of the youngest billionaires,  \n",
      "Antitrust scrutiny began in the late 1990s, resulting in legal battles,  \n",
      "In 2001, Microsoft launched Windows XP, which became widely popular,  \n",
      "The company expanded into hardware with the Xbox in 2001,  \n",
      "In the 2010s, Microsoft shifted focus towards cloud computing and services,  \n",
      "The acquisition of LinkedIn in 2016 marked a significant investment in social networking,  \n",
      "Satya Nadella became CEO in 2014, driving a culture of innovation,  \n",
      "Microsoft has continued to grow, with a strong emphasis on AI and cloud services,  \n",
      "Today, it remains one of the world's largest technology companies,  \n",
      "With a diverse portfolio that includes software, hardware, and cloud solutions."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the request settings\n",
    "req_settings = kernel.get_prompt_execution_settings_from_service_id(service_id)\n",
    "req_settings.max_tokens = 2000\n",
    "req_settings.temperature = 0.7\n",
    "req_settings.top_p = 0.8\n",
    "\n",
    "prompt=\"tell me about the history of {{$company_name}} as TLDR in exactly 200 words. make line seperated by commas. \\n\\nTLDR:\"\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"tldr\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    execution_settings=req_settings,\n",
    ")\n",
    "\n",
    "function = kernel.add_function(\n",
    "    function_name=\"tldr_function\",\n",
    "    plugin_name=\"tldr_plugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "# result = await kernel.invoke(function)\n",
    "# print(result) # => Robots must not harm humans.\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "\n",
    "response = kernel.invoke_stream(plugin_name=\"tldr_plugin\", function_name=\"tldr_function\", company_name=\"Microsoft\")\n",
    "all_chunks_str = \"\"\n",
    "print(\"Assistant:> \", end=\"\")\n",
    "async for chunk in response:\n",
    "    if isinstance(chunk[0], StreamingChatMessageContent) and chunk[0].role == AuthorRole.ASSISTANT:\n",
    "        all_chunks_str += str(chunk[0])\n",
    "        print(str(chunk[0]), end=\"\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0c277",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.2 running a prompt with plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34af3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin = kernel.add_plugin(parent_directory=\"prompt_template_samples/\", plugin_name=\"FunPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d008b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the time traveler bring a ladder to the dinosaur age?\n",
      "\n",
      "Because they heard the T-Rex was a little \"short-tempered\" and wanted to reach new heights in their friendship!\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "joke_function = plugin[\"Joke\"]\n",
    "\n",
    "joke = await kernel.invoke(\n",
    "    joke_function,\n",
    "    KernelArguments(input=\"time travel to dinosaur age\", style=\"super silly\"),\n",
    ")\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece006e",
   "metadata": {},
   "source": [
    "## 🧪 Case 1.3 running a prompt with web search plugin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee4e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the chat bot!    \n",
      "  Type 'exit' to exit.    \n",
      "  Try to search weather, news and more using bing search tool.\n",
      "SearchChatbot:> How can I assist you today? If you have any questions or need information, feel free to ask!\n",
      "Calling Bing search with arguments:\n",
      "  Query: \"current weather in South Korea\"\n",
      "Bing search completed.\n",
      "SearchChatbot:> The current weather in South Korea, specifically in Seoul, is approximately 46°F and mostly cloudy. You can expect light showers and overcast conditions throughout the day. If you need more specific information or forecasts for other cities in South Korea, just let me know!\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "import semantic_kernel.connectors.search.bing\n",
    "kernel = Kernel()\n",
    "kernel.add_service(AzureChatCompletion(service_id=\"chat\"))\n",
    "bing_search_plugin = kernel.add_plugin(KernelPlugin.from_text_search_with_search(\n",
    "        semantic_kernel.connectors.search.bing.BingSearch(),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web information using bing search.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=2,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"site\",\n",
    "                description=\"The site to search.\",\n",
    "                default_value=\"\",\n",
    "                type=\"str\",\n",
    "                is_required=False,\n",
    "                type_object=str,\n",
    "            ),\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "chat_function = kernel.add_function(\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\",\n",
    ")\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"chat\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    function_choice_behavior=FunctionChoiceBehavior.Auto(auto_invoke=True),\n",
    ")\n",
    "\n",
    "history = ChatHistory()\n",
    "system_message = \"\"\"\n",
    "You are a chat bot, use bing plugin to find answers.\n",
    "\"\"\"\n",
    "history.add_system_message(system_message)\n",
    "history.add_user_message(\"Hi there, who are you?\")\n",
    "history.add_assistant_message(\"I am Information Finder, a chat bot. use bing plugin to find answers.\")\n",
    "\n",
    "arguments = KernelArguments(settings=execution_settings)\n",
    "\n",
    "@kernel.filter(filter_type=FilterTypes.FUNCTION_INVOCATION)\n",
    "async def log_bing_filter(\n",
    "    context: FunctionInvocationContext, next: Callable[[FunctionInvocationContext], Awaitable[None]]\n",
    "):\n",
    "    if context.function.plugin_name == \"bing\":\n",
    "        print(\"Calling Bing search with arguments:\")\n",
    "        if \"query\" in context.arguments:\n",
    "            print(f'  Query: \"{context.arguments[\"query\"]}\"')\n",
    "        if \"count\" in context.arguments:\n",
    "            print(f'  Count: \"{context.arguments[\"count\"]}\"')\n",
    "        if \"skip\" in context.arguments:\n",
    "            print(f'  Skip: \"{context.arguments[\"skip\"]}\"')\n",
    "        await next(context)\n",
    "        print(\"Bing search completed.\")\n",
    "    else:\n",
    "        await next(context)\n",
    "\n",
    "\n",
    "async def chat() -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    arguments[\"user_input\"] = user_input\n",
    "    arguments[\"chat_history\"] = history\n",
    "    result = await kernel.invoke(chat_function, arguments=arguments)\n",
    "    print(f\"SearchChatbot:> {result}\")\n",
    "    history.add_user_message(user_input)\n",
    "    history.add_assistant_message(str(result))\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "chatting = True\n",
    "print(\n",
    "    \"Welcome to the chat bot!\\\n",
    "    \\n  Type 'exit' to exit.\\\n",
    "    \\n  Try to search weather, news and more using bing search tool.\"\n",
    ")\n",
    "while chatting:\n",
    "    chatting = await chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc0df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Bing search with arguments:\n",
      "  Query: \"what is new opening hotels in NYC 2025?\"\n",
      "  Skip: \"0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bing search completed.\n",
      "Luxury Hilton Hotel in NYC reopening in 2025. This historic luxury hotel in the Midtown Manhattan area of New York City is currently undergoing a transformative restoration and is due to reopen on September 1, 2025 (the opening date may change). Read Reviews / Check Prices. New Hotel in SoHo NYC opened in September 2024.,New York is poised to welcome new hotels in New York City and beyond in 2025. Add these five to your must-stay list. Faena New York. Expected to debut in spring 2025, Faena New York will offer visitors stunning views of the High Line and Hudson River from One High Line’s East Tower.,New Hotels in Manhattan NY 2025! Be the first to sleep on new sheets, dine at buzz-worthy restaurants, sip cocktails at the hottest rooftops.,From the reimagining of a legendary hotel in New York to breathtaking, newly-built resorts across virtually untouched landscapes, this is the year of monumental hotel openings.,We have created a list with the best newly opened hotels in New York, USA. Hope you enjoy it. We think it might be the best of its kind around. Tip: Set your dates to see room prices! A list by Mia Dahl & Jens Bengtsson. Updated March 2025. Set your dates to see room prices: Newly opened, November 2024. Luxury Hotel.,The project will include a 240-room The Ritz-Carlton resort (2023 anticipated opening), a 400-room Westin Hotels resort (2023 anticipated opening), a 300-room Autograph Collection resort (2025 anticipated opening), and a 500-room Marriott Hotels resort (2025 anticipated opening).,From the reimagining of a legendary hotel in our home base of New York to breathtaking, newly-built resorts across virtually untouched landscapes, this is the year of monumental hotel openings following an already high bar with the 52 Hotels to Watch for in 2024.,A new luxury hotel in NYC, the Kimpton Rockefeller Center, is set to open in late 2025, bringing a fresh wave of boutique elegance to one of Manhattan’s most iconic locations.,Including tented camps, iconic revamps, beachside playgrounds and city centre retreats, these are some of the most exciting new hotel openings in 2025, hand-picked by the CF Team.,Luxury travel in 2025 just got even more exciting! Uncover the hottest new hotel openings—from dreamy Costa Rican beaches to lavish European escapes. Don’t miss these five-star destinations redefining indulgence—start planning now.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "bing_search_fuction = kernel.get_function(\"bing\", \"search\")\n",
    "\n",
    "search_result = await kernel.invoke(\n",
    "    bing_search_fuction,\n",
    "    KernelArguments(\n",
    "        query=\"what is new opening hotels in NYC 2025?\",\n",
    "        top=10,\n",
    "        skip=0,\n",
    "    ),\n",
    ")\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505e28c",
   "metadata": {},
   "source": [
    "# Use single agent to chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6154e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# The following sample demonstrates how to create a simple,       #\n",
    "# Azure AI agent, ChatCompletionAgent, AzureResponsesAgent        #\n",
    "# that answers questions about a sample menu                      #\n",
    "# using a Semantic Kernel Plugin.                                 #\n",
    "###################################################################\n",
    "\n",
    "\n",
    "# Define a sample plugin for the sample\n",
    "class MenuPlugin:\n",
    "    \"\"\"A sample Menu Plugin used for the concept sample.\"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides a list of specials from the menu.\")\n",
    "    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n",
    "        return \"\"\"\n",
    "        Special Soup: Clam Chowder\n",
    "        Special Salad: Cobb Salad\n",
    "        Special Drink: Chai Tea\n",
    "        \"\"\"\n",
    "\n",
    "    @kernel_function(description=\"Provides the price of the requested menu item.\")\n",
    "    def get_item_price(\n",
    "        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n",
    "    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n",
    "        return \"$9.99\"\n",
    "\n",
    "\n",
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = [\n",
    "    \"Hello\",\n",
    "    \"What is the special soup?\",\n",
    "    \"How much does that cost?\",\n",
    "    \"Thank you\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f952185",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.1 chat with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65844e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n",
      "# Host: Hello! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Is there anything else you'd like to know about the menu?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. If there's anything else you'd like to know, feel free to ask!\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions, feel free to ask. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "# Make sure to set the environment variables for the Azure AI Agent\n",
    "ai_agent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent on the Azure AI agent service\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions about the menu.\",\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "        plugins=[MenuPlugin()],  # Add the plugin to the agent\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "    # for user_input in USER_INPUTS:\n",
    "    #     print(f\"# User: {user_input}\")\n",
    "    #     # 4. Invoke the agent for the specified thread for response\n",
    "    #     async for response in agent.invoke(\n",
    "    #         messages=user_input,\n",
    "    #         thread=thread,\n",
    "    #     ):\n",
    "    #         print(f\"# {response.name}: {response}\")\n",
    "    #         thread = response.thread\n",
    "    \n",
    "    # await thread.delete() if thread else None    \n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "    try:\n",
    "        for user_input in USER_INPUTS:\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            first_chunk = True\n",
    "            # 4. Invoke the agent for the current message and print the response\n",
    "            async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "                thread = response.thread\n",
    "                if first_chunk:\n",
    "                    print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                    first_chunk = False\n",
    "                print(response.content, end=\"\", flush=True)\n",
    "            print()\n",
    "    finally:\n",
    "            # Cleanup: Delete the thread and agent\n",
    "            await thread.delete() if thread else None\n",
    "            await client.agents.delete_agent(agent.id)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024625db",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.2 chat with ChatCompletionAgent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68221ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n",
      "# Host: Hello! How can I assist you today?"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Would you like to know more about it or anything else?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Would you like to know more about the menu or anything else?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions or need assistance, feel free to ask. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions based on web search\",\n",
    "        plugins=[MenuPlugin()],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "\n",
    "# for user_input in USER_INPUTS:\n",
    "#     print(f\"# User: {user_input}\")\n",
    "#     # 4. Invoke the agent for a response\n",
    "#     response = await agent.get_response(messages=user_input, thread=thread)\n",
    "#     print(f\"# {response.name}: {response} \")\n",
    "#     thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        first_chunk = True\n",
    "        # 4. Invoke the agent for the current message and print the response\n",
    "        async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "            thread = response.thread\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(response.content, end=\"\", flush=True)\n",
    "        print()\n",
    "\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bbee4",
   "metadata": {},
   "source": [
    "## 🧪 Case 2.3 chat with ResponseAgent (OpenAI Responses API)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f82cb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Hello'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: Hi there! How can I assist you today?\n",
      "# User: 'What is the special soup?'\n",
      "# Host: The special soup is Clam Chowder. Would you like to know more about it?\n",
      "# User: 'How much does that cost?'\n",
      "# Host: The Clam Chowder costs $9.99. Would you like to order anything else?\n",
      "# User: 'Thank you'\n",
      "# Host: You're welcome! If you have any more questions or need assistance, feel free to ask. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "client, model = AzureResponsesAgent.setup_resources()\n",
    "\n",
    "# 2. Create a Semantic Kernel agent for the OpenAI Responses API\n",
    "agent = AzureResponsesAgent(\n",
    "    ai_model_id=model,\n",
    "    client=client,\n",
    "    instructions=\"Answer questions about the menu.\",\n",
    "    name=\"Host\",\n",
    "    plugins=[MenuPlugin()],\n",
    ")\n",
    "\n",
    "# 3. Create a thread for the agent\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ResponsesAgentThread = None\n",
    "\n",
    "########################################\n",
    "# invoke mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#     print(f\"# User: '{user_input}'\")\n",
    "#     # 4. Invoke the agent for the current message and print the response\n",
    "#     response = await agent.get_response(messages=user_input, thread=thread)\n",
    "#     print(f\"# {response.name}: {response.content}\")\n",
    "#     thread = response.thread\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        first_chunk = True\n",
    "        # 4. Invoke the agent for the current message and print the response\n",
    "        async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "            thread = response.thread\n",
    "            if first_chunk:\n",
    "                print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                first_chunk = False\n",
    "            print(response.content, end=\"\", flush=True)\n",
    "        print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c01be",
   "metadata": {},
   "source": [
    "# Use tools to answer specific questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8d674",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.1 code interpreter with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c502bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.'\n",
      "# Agent: def fibonacci_sequence_below_n(max_value):\n",
      "    fib_sequence = []\n",
      "    a, b = 0, 1\n",
      "    while a < max_value:\n",
      "        fib_sequence.append(a)\n",
      "        a, b = b, a + b\n",
      "    return fib_sequence\n",
      "\n",
      "# Get the Fibonacci sequence values less than 101\n",
      "fibonacci_below_101 = fibonacci_sequence_below_n(101)\n",
      "fibonacci_below_101\n",
      "# Agent: The values in the Fibonacci sequence that are less than 101 are:\n",
      "\n",
      "\\[ 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 \\]\n"
     ]
    }
   ],
   "source": [
    "TASK = \"Use code to determine the values in the Fibonacci sequence that that are less then the value of 101.\"\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Create an agent with a code interpreter on the Azure AI agent service\n",
    "    code_interpreter = CodeInterpreterTool()\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=code_interpreter.definitions,\n",
    "        tool_resources=code_interpreter.resources,\n",
    "    )\n",
    "\n",
    "    # 2. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 3. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        print(f\"# User: '{TASK}'\")\n",
    "        # 4. Invoke the agent for the specified thread for response\n",
    "        async for response in agent.invoke(messages=TASK, thread=thread):\n",
    "            if response.role != AuthorRole.TOOL:\n",
    "                print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "    finally:\n",
    "        # 6. Cleanup: Delete the thread and agent\n",
    "        await thread.delete() if thread else None\n",
    "        await client.agents.delete_agent(agent.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22764b",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.2 open API with Azure AI Agent \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f81c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'What is the name and population of the country that uses currency with abbreviation KRW'\n",
      "# Agent: The currency with the abbreviation KRW is the South Korean Won. The country that uses the South Korean Won is South Korea. As of my last update, the estimated population of South Korea is approximately 52 million people. For the most current and precise population figures, consulting a reliable source such as a government database or a world population review would be ideal.\n",
      "# User: 'What is the current weather in the capital city of the country?'\n",
      "# Agent: The current weather in Seoul, the capital of South Korea, is as follows:\n",
      "\n",
      "- **Temperature:** 12°C (54°F)\n",
      "- **Feels Like:** 11°C (53°F)\n",
      "- **Weather Condition:** Clear\n",
      "- **Humidity:** 58%\n",
      "- **Wind:** WNW at 8 km/h (5 mph)\n",
      "- **Visibility:** 10 km\n",
      "- **Pressure:** 1007 mb\n",
      "\n",
      "The weather is characteristically clear with a gentle wind from the west-northwest.\n"
     ]
    }
   ],
   "source": [
    "USER_INPUTS = [\n",
    "    \"What is the name and population of the country that uses currency with abbreviation KRW\",\n",
    "    \"What is the current weather in the capital city of the country?\",\n",
    "]\n",
    "\n",
    "ai_agent_settings = AzureAIAgentSettings.create()\n",
    "\n",
    "async with (\n",
    "    DefaultAzureCredential() as creds,\n",
    "    AzureAIAgent.create_client(credential=creds) as client,\n",
    "):\n",
    "    # 1. Read in the OpenAPI spec files\n",
    "    openapi_spec_file_path = \"resources\"\n",
    "    with open(os.path.join(openapi_spec_file_path, \"weather.json\")) as weather_file:\n",
    "        weather_openapi_spec = json.loads(weather_file.read())\n",
    "    with open(os.path.join(openapi_spec_file_path, \"countries.json\")) as countries_file:\n",
    "        countries_openapi_spec = json.loads(countries_file.read())\n",
    "\n",
    "    # 2. Create OpenAPI tools\n",
    "    # Note that connection or managed identity auth setup requires additional setup in Azure\n",
    "    auth = OpenApiAnonymousAuthDetails()\n",
    "    openapi_weather = OpenApiTool(\n",
    "        name=\"get_weather\",\n",
    "        spec=weather_openapi_spec,\n",
    "        description=\"Retrieve weather information for a location\",\n",
    "        auth=auth,\n",
    "    )\n",
    "    openapi_countries = OpenApiTool(\n",
    "        name=\"get_country\",\n",
    "        spec=countries_openapi_spec,\n",
    "        description=\"Retrieve country information\",\n",
    "        auth=auth,\n",
    "    )\n",
    "\n",
    "    # 3. Create an agent on the Azure AI agent service with the OpenAPI tools\n",
    "    agent_definition = await client.agents.create_agent(\n",
    "        model=ai_agent_settings.model_deployment_name,\n",
    "        tools=openapi_weather.definitions + openapi_countries.definitions,\n",
    "    )\n",
    "\n",
    "    # 4. Create a Semantic Kernel agent for the Azure AI agent\n",
    "    agent = AzureAIAgent(\n",
    "        client=client,\n",
    "        definition=agent_definition,\n",
    "    )\n",
    "\n",
    "    # 5. Create a thread for the agent\n",
    "    # If no thread is provided, a new thread will be\n",
    "    # created and returned with the initial response\n",
    "    thread: AzureAIAgentThread = None\n",
    "\n",
    "    try:\n",
    "        for user_input in USER_INPUTS:\n",
    "            print(f\"# User: '{user_input}'\")\n",
    "            # 7. Invoke the agent for the specified thread for response\n",
    "            async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "                if response.role != AuthorRole.TOOL:\n",
    "                    print(f\"# Agent: {response}\")\n",
    "                thread = response.thread\n",
    "    finally:\n",
    "        # 8. Cleanup: Delete the thread and agent\n",
    "        await client.agents.delete_thread(thread.id)\n",
    "        await client.agents.delete_agent(agent.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be56f4a",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.3 bing search API as plugin with ChatCompletionAgent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c636c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: What is today's weather in South Korea?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Host: Today in South Korea, the weather is mostly clear and pleasant. In Seoul, the current temperature is around 21°C (approximately 69.8°F), with a predicted minimum of 13°C (55.4°F) tonight. The maximum temperature is expected to be around 16°C, and temperatures will drop to about 10°C in the evening.\n",
      "\n",
      "For more details, you can check the latest updates [here](https://www.weather.com). \n"
     ]
    }
   ],
   "source": [
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = [\n",
    "    \"What is today's weather in South Korea?\"\n",
    "]\n",
    "from semantic_kernel.connectors.search.bing import BingSearch\n",
    "from semantic_kernel.functions import KernelArguments, KernelParameterMetadata, KernelPlugin\n",
    "\n",
    "webplugin = KernelPlugin.from_text_search_with_search(\n",
    "        BingSearch(api_key=os.getenv(\"BING_SUBSCRIPTION_KEY\")),\n",
    "        plugin_name=\"bing\",\n",
    "        description=\"Search the web for information.\",\n",
    "        parameters=[\n",
    "            KernelParameterMetadata(\n",
    "                name=\"query\",\n",
    "                description=\"The search query.\",\n",
    "                type=\"str\",\n",
    "                is_required=True,\n",
    "                type_object=str,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"top\",\n",
    "                description=\"The number of results to return.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=1,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            KernelParameterMetadata(\n",
    "                name=\"skip\",\n",
    "                description=\"The number of results to skip.\",\n",
    "                type=\"int\",\n",
    "                is_required=False,\n",
    "                default_value=0,\n",
    "                type_object=int,\n",
    "            ),\n",
    "            # KernelParameterMetadata(\n",
    "            #     name=\"site\",\n",
    "            #     description=\"The site to search.\",\n",
    "            #     default_value=\"https://github.com/microsoft/semantic-kernel/tree/main/python\",\n",
    "            #     type=\"str\",\n",
    "            #     is_required=False,\n",
    "            #     type_object=str,\n",
    "            # ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "agent = ChatCompletionAgent(\n",
    "        service=AzureChatCompletion(),\n",
    "        name=\"Host\",\n",
    "        instructions=\"Answer questions from web search results. Add the web search reference url to the answer.\",\n",
    "        plugins=[webplugin],\n",
    "    )\n",
    "\n",
    "# 2. Create a thread to hold the conversation\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: ChatHistoryAgentThread = None\n",
    "\n",
    "for user_input in USER_INPUTS:\n",
    "    print(f\"# User: {user_input}\")\n",
    "    # 4. Invoke the agent for a response\n",
    "    response = await agent.get_response(messages=user_input, thread=thread)\n",
    "    print(f\"# {response.name}: {response} \")\n",
    "    thread = response.thread\n",
    "\n",
    "\n",
    "########################################\n",
    "# invoke_stream mode \n",
    "########################################\n",
    "# for user_input in USER_INPUTS:\n",
    "#         print(f\"# User: '{user_input}'\")\n",
    "#         first_chunk = True\n",
    "#         # 4. Invoke the agent for the current message and print the response\n",
    "#         async for response in agent.invoke_stream(messages=user_input, thread=thread):\n",
    "#             thread = response.thread\n",
    "#             if first_chunk:\n",
    "#                 print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "#                 first_chunk = False\n",
    "#             print(response.content, end=\"\", flush=True)\n",
    "#         print()\n",
    "\n",
    "await thread.delete() if thread else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb7922",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.4 file search tool with AzureAssistantAgent (Open AI Asisstant)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88bb7af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User: 'Who works in sales?'\n",
      "# Agent: In the sales department, the following individuals work:\n",
      "\n",
      "1. **Mariam Jaslyn** - Sales representative\n",
      "2. **Angelino Embla** - Sales representative\n",
      "3. **Hicran Bea** - Sales manager【4:0†source】.\n",
      "# User: 'Who is the youngest employee?'\n",
      "# Agent: The youngest employee is **Teodor Britton**, who is an accountant born on January 9, 1997【8:0†source】.\n",
      "# User: 'I have a customer request, who can help me?'\n",
      "# Agent: For assistance with a customer request, you can reach out to the following individuals:\n",
      "\n",
      "1. **Mariam Jaslyn** - Sales representative\n",
      "2. **Hicran Bea** - Sales manager\n",
      "3. **Angelino Embla** - Sales representative【12:0†source】.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulate a conversation with the agent\n",
    "USER_INPUTS = {\n",
    "    \"Who is the youngest employee?\",\n",
    "    \"Who works in sales?\",\n",
    "    \"I have a customer request, who can help me?\",\n",
    "}\n",
    "\n",
    "\n",
    "# 1. Create the client using Azure OpenAI resources and configuration\n",
    "client, model = AzureAssistantAgent.setup_resources()\n",
    "\n",
    "# 2. Read and upload the file to the Azure OpenAI assistant service\n",
    "pdf_file_path = \"resources/employees.pdf\"\n",
    "\n",
    "\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    file = await client.files.create(file=file, purpose=\"assistants\")\n",
    "\n",
    "vector_store = await client.vector_stores.create(\n",
    "    name=\"step4_assistant_file_search\",\n",
    "    file_ids=[file.id],\n",
    ")\n",
    "\n",
    "# 3. Create file search tool with uploaded resources\n",
    "file_search_tool, file_search_tool_resources = AzureAssistantAgent.configure_file_search_tool(vector_store.id)\n",
    "\n",
    "# 4. Create the assistant on the Azure OpenAI service with the file search tool\n",
    "definition = await client.beta.assistants.create(\n",
    "    model=model,\n",
    "    instructions=\"Find answers to the user's questions in the provided file.\",\n",
    "    name=\"FileSearch\",\n",
    "    tools=file_search_tool,\n",
    "    tool_resources=file_search_tool_resources,\n",
    ")\n",
    "\n",
    "# 5. Create a Semantic Kernel agent for the Azure OpenAI assistant\n",
    "agent = AzureAssistantAgent(\n",
    "    client=client,\n",
    "    definition=definition,\n",
    ")\n",
    "\n",
    "# 6. Create a new thread for use with the assistant\n",
    "# If no thread is provided, a new thread will be\n",
    "# created and returned with the initial response\n",
    "thread: AssistantAgentThread = None\n",
    "\n",
    "try:\n",
    "    for user_input in USER_INPUTS:\n",
    "        print(f\"# User: '{user_input}'\")\n",
    "        # 7. Invoke the agent for the current thread and print the response\n",
    "        async for response in agent.invoke(messages=user_input, thread=thread):\n",
    "            print(f\"# Agent: {response}\")\n",
    "            thread = response.thread\n",
    "finally:\n",
    "    # 9. Clean up the resources\n",
    "    await client.files.delete(file.id)\n",
    "    await client.vector_stores.delete(vector_store.id)\n",
    "    await client.beta.threads.delete(thread.id)\n",
    "    await client.beta.assistants.delete(agent.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d8159",
   "metadata": {},
   "source": [
    "## 🧪 Case 3.5 BingGrounding with Azure AI Agent (Optional)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6ed132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created message, ID: msg_ruQneTcLMXkt2ZWIjlNwMpKu\n",
      "Messages: {'object': 'list', 'data': [{'id': 'msg_CSsrrCXmvSEef9FhAntzff43', 'object': 'thread.message', 'created_at': 1744288188, 'assistant_id': 'asst_DL4OpkzoeKQFVq6kdB9Or2Gk', 'thread_id': 'thread_iXWOLWzxf0LL8yJzB8u5KnoV', 'run_id': 'run_RQOvfFGTJmlDUXRPmkLi84Rf', 'role': 'assistant', 'content': [{'type': 'text', 'text': {'value': 'Today, April 10, 2025, in South Korea, the weather in Seoul is experiencing light rain with temperatures ranging from 18°C during the day to 10°C at night【3:0†source】.', 'annotations': [{'type': 'url_citation', 'text': '【3:0†source】', 'start_index': 154, 'end_index': 166, 'url_citation': {'url': 'https://www.weather25.com/asia/south-korea/seoul?page=month&month=April', 'title': 'Seoul weather in April 2025 | Seoul 14 day weather'}}]}}], 'attachments': [], 'metadata': {}}, {'id': 'msg_ruQneTcLMXkt2ZWIjlNwMpKu', 'object': 'thread.message', 'created_at': 1744288186, 'assistant_id': None, 'thread_id': 'thread_iXWOLWzxf0LL8yJzB8u5KnoV', 'run_id': None, 'role': 'user', 'content': [{'type': 'text', 'text': {'value': \"What is today's weather in South Korea?\", 'annotations': []}}], 'attachments': [], 'metadata': {}}], 'first_id': 'msg_CSsrrCXmvSEef9FhAntzff43', 'last_id': 'msg_ruQneTcLMXkt2ZWIjlNwMpKu', 'has_more': False}\n",
      "Message ID: msg_CSsrrCXmvSEef9FhAntzff43\n",
      "Role: assistant\n",
      "Content: Today, April 10, 2025, in South Korea, the weather in Seoul is experiencing light rain with temperatures ranging from 18°C during the day to 10°C at night【3:0†source】.\n",
      "Created At: 1744288188\n",
      "Metadata: {}\n",
      "URL: https://www.weather25.com/asia/south-korea/seoul?page=month&month=April\n",
      "Title: Seoul weather in April 2025 | Seoul 14 day weather\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects.models import BingGroundingTool\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "USER_INPUTS = [\n",
    "    \"What is today's weather in South Korea?\",\n",
    "    \"What is the new hotels in NYC 2025?\",\n",
    "]\n",
    "\n",
    "creds = DefaultAzureCredential()\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=creds,\n",
    "    conn_str=os.getenv(\"AZURE_AI_AGENT_PROJECT_CONNECTION_STRING\"),\n",
    ")\n",
    "bing_connection = project_client.connections.get(\n",
    "    connection_name=os.getenv(\"BING_GROUNDING_CONNECTION_NAME\"),\n",
    ")\n",
    "conn_id = bing_connection.id\n",
    "\n",
    "\n",
    "# 1. Create an agent with a code interpreter on the Azure AI agent service\n",
    "bing = BingGroundingTool(connection_id=conn_id)\n",
    "with project_client:\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=os.getenv(\"AZURE_AI_AGENT_MODEL_DEPLOYMENT_NAME\"),\n",
    "        name=\"my-bing-agent\",\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        tools=bing.definitions,\n",
    "        headers={\"x-ms-enable-preview\": \"true\"}\n",
    "    )\n",
    "    #if you want to use existing agent, you can use the following code\n",
    "    # agent = project_client.agents.get_agent(agent_id=\"your agent id\")\n",
    "    \n",
    "    # Create thread for communication\n",
    "    thread = project_client.agents.create_thread()\n",
    "    \n",
    "    # Create message to thread\n",
    "    message = project_client.agents.create_message(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=USER_INPUTS[0],\n",
    "    )\n",
    "    print(f\"Created message, ID: {message.id}\")\n",
    "\n",
    "    run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "    \n",
    "    run_steps = project_client.agents.list_run_steps(run_id=run.id, thread_id=thread.id)\n",
    "    run_steps_data = run_steps['data']\n",
    "    \n",
    "    if run.status == \"failed\":\n",
    "        print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "    # Delete the assistant when done\n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    \n",
    "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "    print(f\"Messages: {messages}\")\n",
    "    \n",
    "    for msg in messages['data']:\n",
    "        if msg['role'] == 'assistant':\n",
    "            print(f\"Message ID: {msg['id']}\")\n",
    "            print(f\"Role: {msg['role']}\")\n",
    "            print(f\"Content: {msg['content'][0]['text']['value']}\")\n",
    "            print(f\"Created At: {msg['created_at']}\")\n",
    "            print(f\"Metadata: {msg['metadata']}\")\n",
    "            \n",
    "            # Print URL information if available\n",
    "            annotations = msg['content'][0]['text'].get('annotations', [])\n",
    "            for annotation in annotations:\n",
    "                if annotation['type'] == 'url_citation':\n",
    "                    url_info = annotation['url_citation']\n",
    "                    print(f\"URL: {url_info['url']}\")\n",
    "                    print(f\"Title: {url_info['title']}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fbb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
