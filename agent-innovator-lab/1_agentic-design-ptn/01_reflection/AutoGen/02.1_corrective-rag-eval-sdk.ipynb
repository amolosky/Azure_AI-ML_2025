{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26ab996",
   "metadata": {},
   "source": [
    "# Corrective RAG (CRAG)\n",
    "---\n",
    "\n",
    "### What is Corrective RAG?\n",
    "\n",
    "Corrective RAG (CRAG) is a methodology that adds a step to the RAG (Retrieval Augmented Generation) strategy to evaluate the documents found during the search process and refine the knowledge. This includes a series of processes to check the search results before generation and, if necessary, perform auxiliary searches to generate high-quality answers.\n",
    "\n",
    "- Retrieval Grader: Evaluates the relevance of retrieved documents and assigns a score to each document.\n",
    "- Web Search Integration: If quality of retrieved documents is low, CRAG uses web searches to augment retrieval results. It optimizes search results through query rewriting.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [Corrective RAG paper](https://arxiv.org/pdf/2401.15884)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6458235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.ai.evaluation import GroundednessEvaluator, RelevanceEvaluator, RetrievalEvaluator\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from autogen_core.models import ChatCompletionClient, SystemMessage, UserMessage, AssistantMessage\n",
    "from autogen_core import MessageContext, RoutedAgent, SingleThreadedAgentRuntime, TopicId, message_handler, type_subscription\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the environment variables\n",
    "azure_ai_search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "search_credential = AzureKeyCredential(os.getenv(\"AZURE_AI_SEARCH_API_KEY\", \"\")) if len(os.getenv(\"AZURE_AI_SEARCH_API_KEY\", \"\")) > 0 else DefaultAzureCredential()\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"hotels-sample-index\")\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\") if len(os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")) > 0 else None\n",
    "azure_openai_chat_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\", \"text-embedding-ada-002\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")\n",
    "\n",
    "bing_subscription_key = os.getenv(\"BING_SUBSCRIPTION_KEY\", \"\") if len(os.getenv(\"BING_SUBSCRIPTION_KEY\", \"\")) > 0 else None\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": azure_openai_endpoint,\n",
    "    \"api_key\": azure_openai_key,\n",
    "    \"azure_deployment\": azure_openai_chat_deployment_name,\n",
    "    \"api_version\": azure_openai_api_version,\n",
    "    \"type\": \"azure_openai\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec192a9",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 1. Test and Construct each module\n",
    "---\n",
    "\n",
    "Before building the entire the graph pipeline, we will test and construct each module separately.\n",
    "\n",
    "- **SearchClient(Retrieval)**\n",
    "- **Retrieval Grader**\n",
    "- **Answer Generator**\n",
    "- **Question Re-writer**\n",
    "- **Web Search Tool**\n",
    "\n",
    "### Construct Retrieval Chain based on PDF\n",
    "- We use the hotels-sample-index, which can be created in minutes and runs on any search service tier. This index is created by a wizard using built-in sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_ai_search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
    "azure_search_admin_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\", \"\")\n",
    "search_client = SearchClient(\n",
    "    endpoint=azure_ai_search_endpoint,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(azure_search_admin_key),\n",
    "    semantic_configuration_name='my-semantic-config', \n",
    ")\n",
    "\n",
    "# Query is the question being asked. It's sent to the search engine and the LLM.\n",
    "query=\"Can you recommend a few hotels with complimentary breakfast?\"\n",
    "\n",
    "fields = \"descriptionVector\" # TODO: Check if this is the correct field name\n",
    "# don't use exhaustive search for large indexes\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=2, fields=fields, exhaustive=True)\n",
    "\n",
    "# Search results are created by the search client.\n",
    "# Search results are composed of the top 3 results and the fields selected from the search index.\n",
    "# Search results include the top 3 matches to your query.\n",
    "search_results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries= [vector_query],\n",
    "    select=\"Description,HotelName,Tags\",\n",
    "    top=3,\n",
    ")\n",
    "sources_formatted = \"\\n\".join([f'{document[\"HotelName\"]}:{document[\"Description\"]}:{document[\"Tags\"]}' for document in search_results])\n",
    "\n",
    "print(sources_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d39ad",
   "metadata": {},
   "source": [
    "### Define your LLM\n",
    "\n",
    "This hands-on only uses the `gpt-4o-mini`, but you can utilize multiple models in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aoai_client = AzureOpenAI(\n",
    "#     azure_endpoint=azure_openai_endpoint,\n",
    "#     api_key=azure_openai_key,\n",
    "#     api_version=openai_api_version,\n",
    "# )\n",
    "\n",
    "# This is not the same object as the one above. This is the client that is used to interact with the Azure OpenAI Chat API.\n",
    "autogen_aoai_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    model = azure_openai_chat_deployment_name,\n",
    "    api_version=azure_openai_api_version,\n",
    "    api_key=azure_openai_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec10e5",
   "metadata": {},
   "source": [
    "### Question-Retrieval Grader\n",
    "\n",
    "Construct a retrieval grader that evaluates the relevance of the retrieved documents to the input question. The retrieval grader should take the input question and the retrieved documents as input and output a relevance score for each document.<br>\n",
    "Note that the retrieval grader should be able to handle **multiple documents** as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_eval  = RetrievalEvaluator(model_config)\n",
    "\n",
    "query_response = dict(\n",
    "    query=query,\n",
    "    context=sources_formatted\n",
    ")\n",
    "\n",
    "relevance_score = retrieval_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(relevance_score)\n",
    "relevance_score['retrieval']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b7f87",
   "metadata": {},
   "source": [
    "### Answer Generator\n",
    "\n",
    "Construct a LLM Generation node. This is a Naive RAG chain that generates an answer based on the retrieved documents. \n",
    "\n",
    "We recommend you to use more advanced RAG chain for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class HotelInfo(BaseModel):\n",
    "    hotel_name: str\n",
    "    description: str\n",
    "\n",
    "class RecommendationList(BaseModel):\n",
    "    recommendation: List[HotelInfo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prompt provides instructions to the model\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are a friendly assistant that recommends hotels based on activities and amenities.\n",
    "Answer the query using only the context provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of context below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Generate a response that includes the top 3 results.\n",
    "Do not generate answers that don't use the context below.\n",
    "Query: {query}\n",
    "Context:\\n{context}\n",
    "\"\"\"\n",
    "\n",
    "# Send the search results and the query to the LLM to generate a response based on the prompt.\n",
    "response = await autogen_aoai_client.create(\n",
    "        messages = [\n",
    "        UserMessage(content=GROUNDED_PROMPT.format(query=query, context=sources_formatted), source=\"user\"),\n",
    "    ],\n",
    "        extra_create_args={\"response_format\": RecommendationList},\n",
    ")\n",
    "\n",
    "response_content = json.loads(response.content)\n",
    "for recommendation in response_content['recommendation']:\n",
    "    print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05275c",
   "metadata": {},
   "source": [
    "### Keyword Re-writer\n",
    "\n",
    "Construct a `keyword_rewriter` agent to rewrite the question as the search keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Can you recommend a few hotels with complimentary breakfast?\"\n",
    "\n",
    "# This prompt provides instructions to the model\n",
    "KEYWORD_REWRITE_PROMPT=\"\"\"\n",
    "You a keyword re-writer that converts an input question to a better version that is optimized for search. \n",
    "Generate search keyword from a user query \n",
    "to be more specific, detailed, and likely to retrieve relevant information, allowing for a more accurate response through web search.\n",
    "Don't include the additional context from the user question.\n",
    "\n",
    "Query: {query}\n",
    "Revised web search query:\n",
    "\"\"\"\n",
    "\n",
    "# Send the search results and the query to the LLM to generate a response based on the prompt.\n",
    "response = await autogen_aoai_client.create(\n",
    "        messages = [\n",
    "        UserMessage(content=KEYWORD_REWRITE_PROMPT.format(query=query), source=\"user\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Here is the response from the chat model.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7d8b2",
   "metadata": {},
   "source": [
    "### Web Search Tool\n",
    "\n",
    "Web search tool is used to enhance the context. <br>\n",
    "\n",
    "It is used when all the documents do not meet the relevance threshold or the evaluator is not confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5444bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_genai_utils.tools import BingSearch\n",
    "\n",
    "WEB_SEARCH_FORMAT_OUTPUT = False\n",
    "\n",
    "web_search_tool = BingSearch(\n",
    "    max_results=3,\n",
    "    locale=\"en-US\",\n",
    "    include_news=False,\n",
    "    include_entity=False,\n",
    "    format_output=WEB_SEARCH_FORMAT_OUTPUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Newest Openings Hotels in NYC 2024 2025?\"\n",
    "results = web_search_tool.invoke({\"query\": query})\n",
    "print(results[0].get(\"content\", \"No content\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d720d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ðŸ§ª Step 2. Define the Agentic Architecture\n",
    "- Before building the agentic pipeline, we need to design the message, topic, agent and message routing logic. \n",
    "- You should define the terminate condition for the pipeline.\n",
    "\n",
    "### Message, Topic, Agent Definition\n",
    "\n",
    "```markdown\n",
    "```python\n",
    "\n",
    "# Message Definition\n",
    "@dataclass\n",
    "class Message:\n",
    "    query: str = None\n",
    "    context: str = None\n",
    "    response: str = None\n",
    "    source: str = None\n",
    "\n",
    "\n",
    "# Topic Definition\n",
    "user_query_topic_type = \"UserQueryTopic\"\n",
    "rewrite_topic_type = \"RewriteQueryTopic\"\n",
    "generate_topic_type = \"GenerateTopic\"\n",
    "web_search_topic_type = \"WebSearchTopic\"\n",
    "user_topic_type = \"UserAgent\"\n",
    "\n",
    "# Agent Definition\n",
    "class RetrievalGraderAgent(RoutedAgent):\n",
    "class KeywordRewriteAgent(RoutedAgent):\n",
    "class GenerateAgent(RoutedAgent):\n",
    "class WebSearchAgent(RoutedAgent):\n",
    "class UserAgent(RoutedAgent):\n",
    "\n",
    "\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5b5ba",
   "metadata": {},
   "source": [
    "Visualizing the abstract architecture of the pipeline will help you understand the message flow and the agent's role in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad292269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_genai_utils.graphs import visualize_agents\n",
    "\n",
    "agents = [\n",
    "    \"Start\",\n",
    "    \"RetrievalGraderAgent\",\n",
    "    \"KeywordRewriteAgent\",\n",
    "    \"GenerateAgent\",\n",
    "    \"WebSearchAgent\",\n",
    "    \"UserAgent\",\n",
    "]\n",
    "interactions = [\n",
    "    (\"Start\", \"RetrievalGraderAgent\"),\n",
    "    (\"RetrievalGraderAgent\", \"GenerateAgent\", \"Generates Response\"),\n",
    "    (\"RetrievalGraderAgent\", \"KeywordRewriteAgent\", \"Rewrites as keyword for bing search\"),\n",
    "    (\"KeywordRewriteAgent\", \"WebSearchAgent\"),\n",
    "    (\"WebSearchAgent\", \"GenerateAgent\"),\n",
    "    (\"GenerateAgent\", \"UserAgent\"),\n",
    "]\n",
    "\n",
    "visualize_agents(agents, interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a3e9e",
   "metadata": {},
   "source": [
    "This is an example of visualized pipeline\n",
    "\n",
    "![\"corrective-RAG\"](../../images/corrective-RAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    query: str = None\n",
    "    context: str = None\n",
    "    response: str = None\n",
    "    source: str = None\n",
    "    def set_source(self, source: str) -> \"Message\":\n",
    "        self.source = source\n",
    "        return self\n",
    "\n",
    "# Topic Definition\n",
    "user_query_topic_type = \"UserQueryTopic\"\n",
    "keyword_rewrite_topic_type = \"KeywordRewriteAgent\"\n",
    "generate_topic_type = \"GenerateAgent\"\n",
    "web_search_topic_type = \"WebSearchAgent\"\n",
    "user_topic_type = \"UserAgent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883dde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@type_subscription(topic_type=user_query_topic_type)\n",
    "class RAGGraderAgent(RoutedAgent):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            azure_ai_search_endpoint:str, \n",
    "            azure_search_admin_key:str,\n",
    "            index_name: str,\n",
    "            retrieval_evaluator: RetrievalEvaluator,\n",
    "            ) -> None:\n",
    "        \n",
    "        super().__init__(\"RAG Grader Agent\")\n",
    "        self.index_name = index_name\n",
    "        self.azure_ai_search_endpoint = azure_ai_search_endpoint\n",
    "        self.azure_search_admin_key = azure_search_admin_key\n",
    "        self.retrieval_evaluator = retrieval_evaluator\n",
    "\n",
    "    def config_search(self) -> SearchClient:\n",
    "        service_endpoint = self.azure_ai_search_endpoint\n",
    "        key = self.azure_search_admin_key\n",
    "        index_name = self.index_name\n",
    "        credential = AzureKeyCredential(key)\n",
    "        return SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "    async def do_search(self, query: str) -> str:\n",
    "        \"\"\"Search indexed data using Azure Cognitive Search with vector-based queries.\"\"\"\n",
    "        aia_search_client = self.config_search()\n",
    "\n",
    "        fields = \"descriptionVector\" # TODO: Check if this is the correct field name\n",
    "        # don't use exhaustive search for large indexes\n",
    "        vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=fields, exhaustive=True)\n",
    " \n",
    "        search_results = aia_search_client.search(  \n",
    "            search_text=query,  \n",
    "            vector_queries= [vector_query],\n",
    "            select=[\"Description,HotelName,Tags\"], #TODO: Check if these are the correct field names\n",
    "            top=3 #TODO: Check if this is the correct number of results\n",
    "        )\n",
    "        answer = \"\\n\".join([f'{document[\"HotelName\"]}:{document[\"Description\"]}:{document[\"Tags\"]}' for document in search_results])  \n",
    "        return answer\n",
    "    \n",
    "    @message_handler\n",
    "    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n",
    "        print(f\"\\n{'-'*80}\\n{self.id.type} received a message:\\n\")\n",
    "        context_from_ai_search = await self.do_search(message.query)\n",
    "        print(context_from_ai_search)\n",
    "\n",
    "        query_response = dict(\n",
    "            query=query,\n",
    "            context=context_from_ai_search\n",
    "        )\n",
    "\n",
    "        retrieval_score = self.retrieval_evaluator (\n",
    "            **query_response\n",
    "        )\n",
    "\n",
    "        print(f\"retrieval_score: {retrieval_score['retrieval']}\")\n",
    "       \n",
    "        if(retrieval_score[\"retrieval\"] >= 3.0):\n",
    "            await self.publish_message(Message(query=query, context=context_from_ai_search, source=message.source), topic_id=TopicId(type=generate_topic_type, source=message.source))\n",
    "        else:\n",
    "            await self.publish_message(Message(query=query, context=context_from_ai_search, source=message.source), topic_id=TopicId(type=keyword_rewrite_topic_type, source=message.source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452355a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD_REWRITE_PROMPT=\"\"\"\n",
    "You a keyword re-writer that converts an input question to a better version that is optimized for search. \n",
    "Generate search keyword from a user query \n",
    "to be more specific, detailed, and likely to retrieve relevant information, allowing for a more accurate response through web search.\n",
    "Don't include the additional context from the user question.\n",
    "\n",
    "Query: {query}\n",
    "Revised web search query:\n",
    "\"\"\"\n",
    "\n",
    "@type_subscription(topic_type=keyword_rewrite_topic_type)\n",
    "class KeywordRewriteAgent(RoutedAgent):\n",
    "    def __init__(self, model_client: ChatCompletionClient) -> None:\n",
    "        super().__init__(\"Query Rewrite Agent\")\n",
    "        self._system_message = SystemMessage(\n",
    "            content=(\n",
    "                \"\"\"\n",
    "                    You are an helper agent that can rewrite the query.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        self._model_client = model_client\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n",
    "        print(f\"\\n{'-'*80}\\n{self.id.type} received a message:\\n\")\n",
    "        llm_result = await self._model_client.create(\n",
    "            messages=[self._system_message, \n",
    "                        UserMessage(content=KEYWORD_REWRITE_PROMPT.format(query=message.query), source=message.source),\n",
    "                      ],\n",
    "            cancellation_token=ctx.cancellation_token,\n",
    "        )\n",
    "        response = llm_result.content\n",
    "        print(response)\n",
    "        assert isinstance(response, str)\n",
    "        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n",
    "        \n",
    "        await self.publish_message(Message(query=response, context=message.context, source=message.source), topic_id=TopicId(type=web_search_topic_type, source=message.source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCORRECT_ANSWER=\"\"\"\n",
    "Hello, and thank you for bringing this to our attention! I may have provided an inaccurate or misleading response, and I sincerely apologize for the confusion.\n",
    "As an AI, I aim to deliver helpful and accurate information, but sometimes I might misinterpret or generate an incorrect response. Your feedback is invaluable and helps me improve.\n",
    "\n",
    "If you'd like, feel free to share more details or clarify your question, and Iâ€™ll do my best to assist you further. Thank you for your understanding and patience! ðŸ˜Š\n",
    "\"\"\"\n",
    "\n",
    "@type_subscription(topic_type=web_search_topic_type)\n",
    "class WebSearchAgent(RoutedAgent):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            web_search_tool: BingSearch,\n",
    "            retrieval_evaluator: RetrievalEvaluator,\n",
    "            ) -> None:\n",
    "        \n",
    "        super().__init__(\"WebSearch Agent\")\n",
    "        self.web_search_tool = web_search_tool\n",
    "        self.retrieval_evaluator = retrieval_evaluator\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n",
    "        print(f\"\\n{'-'*80}\\n{self.id.type} received a message:\\n\")\n",
    "        search_results = web_search_tool.invoke({\"query\": query})\n",
    "        print(search_results)\n",
    "        try:\n",
    "            contents = []\n",
    "            items = list(search_results)\n",
    "            for i in range(min(3, len(items))):\n",
    "                doc = items[i]\n",
    "                contents.append(doc.get(\"content\", \"No content\"))\n",
    "            content = \"\\n\".join(contents)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            content = \"No content\"\n",
    "        \n",
    "        search_response = dict(\n",
    "            query=message.query,\n",
    "            context=content,\n",
    "            response=message.response\n",
    "            \n",
    "        )\n",
    "\n",
    "        retrieval_score = self.retrieval_evaluator (\n",
    "            **search_response\n",
    "        )\n",
    "        print(f\"retrieval_score: {retrieval_score['retrieval']}\")\n",
    "        if(retrieval_score[\"retrieval\"] < 3.0):\n",
    "            await self.publish_message(AssistantMessage(content=INCORRECT_ANSWER, source=message.source), topic_id=TopicId(type=user_topic_type, source=message.source))\n",
    "\n",
    "        await self.publish_message(Message(query=message.query, context=content, source=message.source), topic_id=TopicId(type=generate_topic_type, source=message.source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prompt provides instructions to the model\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "Answer the query using only the context provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of context below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the context below.\n",
    "Query: {query}\n",
    "Context:\\n{context}\n",
    "\"\"\"\n",
    "\n",
    "@type_subscription(topic_type=generate_topic_type)\n",
    "class GenerateAgent(RoutedAgent):\n",
    "    def __init__(self, model_client: ChatCompletionClient) -> None:\n",
    "        super().__init__(\"Generate Agent\")\n",
    "        self._system_message = SystemMessage(\n",
    "            content=(\n",
    "                \"\"\"\n",
    "                    You are a friendly assistant that recommends hotels based on activities and amenities.\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "        self._model_client = model_client\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n",
    "        print(f\"\\n{'-'*80}\\n{self.id.type} received a message:\\n\")\n",
    "        llm_result = await self._model_client.create(\n",
    "            messages=[self._system_message, \n",
    "                        UserMessage(content=GROUNDED_PROMPT.format(query=message.query, context=message.context), source=message.source),\n",
    "                      ],\n",
    "            extra_create_args={\"response_format\": RecommendationList},\n",
    "            cancellation_token=ctx.cancellation_token,\n",
    "        )\n",
    "\n",
    "        response_content = llm_result.content\n",
    "        print(response_content)\n",
    "        await self.publish_message(AssistantMessage(content=response_content, source=message.source), topic_id=TopicId(type=user_topic_type, source=message.source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@type_subscription(topic_type=user_topic_type)\n",
    "class UserAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"A user agent that outputs the final copy to the user.\")\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_final_copy(self, message: AssistantMessage, ctx: MessageContext) -> None:\n",
    "        print(f\"\\n{'-'*80}\\n{self.id.type} received final copy:\\n\")\n",
    "        assert isinstance(message.content, str)\n",
    "        response_content = json.loads(message.content)\n",
    "        for recommendation in response_content['recommendation']:\n",
    "            print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8fce6e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ðŸ§ª Step 3. Execute the Workflow\n",
    "---\n",
    "\n",
    "### Execute the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c329af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "await RAGGraderAgent.register(runtime, type=user_query_topic_type, factory=lambda: RAGGraderAgent(\n",
    "    azure_ai_search_endpoint=azure_ai_search_endpoint,\n",
    "    azure_search_admin_key=azure_search_admin_key,\n",
    "    index_name=index_name,\n",
    "    retrieval_evaluator=RetrievalEvaluator(model_config),\n",
    "    ))\n",
    "\n",
    "await KeywordRewriteAgent.register(runtime, type=keyword_rewrite_topic_type, factory=lambda: KeywordRewriteAgent(model_client=autogen_aoai_client))\n",
    "\n",
    "await GenerateAgent.register(runtime, type=generate_topic_type, factory=lambda: GenerateAgent(model_client=autogen_aoai_client))\n",
    "\n",
    "WEB_SEARCH_FORMAT_OUTPUT = False\n",
    "\n",
    "await WebSearchAgent.register(runtime, type=web_search_topic_type, factory=lambda: WebSearchAgent(\n",
    "    web_search_tool=BingSearch(\n",
    "        max_results=3,\n",
    "        locale=\"en-US\",\n",
    "        include_news=False,\n",
    "        include_entity=False,\n",
    "        format_output=WEB_SEARCH_FORMAT_OUTPUT,\n",
    "    ),\n",
    "    retrieval_evaluator=RetrievalEvaluator(model_config),\n",
    "    ))\n",
    "\n",
    "\n",
    "await UserAgent.register(runtime, type=user_topic_type, factory=lambda: UserAgent())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32562244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "runtime.start()\n",
    "\n",
    "await runtime.publish_message(Message(query=\"Can you recommend the newest Openings Hotels in Manhattan Midtown 2024?\", source=\"User\"), topic_id=TopicId(type=user_query_topic_type, source=\"user\"))\n",
    "\n",
    "await runtime.stop_when_idle()\n",
    "\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda1a04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eea3e223",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
