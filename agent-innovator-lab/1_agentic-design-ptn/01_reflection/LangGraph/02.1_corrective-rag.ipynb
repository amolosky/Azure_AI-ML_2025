{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1961db5f",
   "metadata": {},
   "source": [
    "# Corrective RAG (CRAG)\n",
    "---\n",
    "\n",
    "### What is Corrective RAG?\n",
    "\n",
    "Corrective RAG (CRAG) is a methodology that adds a step to the RAG (Retrieval Augmented Generation) strategy to evaluate the documents found during the search process and refine the knowledge. This includes a series of processes to check the search results before generation and, if necessary, perform auxiliary searches to generate high-quality answers.\n",
    "\n",
    "- Retrieval Grader: Evaluates the relevance of retrieved documents and assigns a score to each document.\n",
    "- Web Search Integration: If quality of retrieved documents is low, CRAG uses web searches to augment retrieval results. It optimizes search results through query rewriting.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [Corrective RAG paper](https://arxiv.org/pdf/2401.15884)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3375ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure_genai_utils.tracer import get_langchain_api_key, set_langsmith\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# If you want to trace your RAG API calls, please set the tracing=True. You need to have a valid Langchain API key.\n",
    "langchain_key, has_langchain_key = get_langchain_api_key()\n",
    "set_langsmith(\"[RAG Innv Lab] 1_Agentic-Design-Pattern\", tracing=False)\n",
    "\n",
    "azure_openai_chat_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d52b6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ðŸ§ª Step 1. Test and Construct each module\n",
    "---\n",
    "\n",
    "Before building the entire the graph pipeline, we will test and construct each module separately.\n",
    "\n",
    "- **Retrieval Grader**\n",
    "- **Answer Generator**\n",
    "- **Question Re-writer**\n",
    "- **Web Search Tool**\n",
    "\n",
    "### Construct Retrieval Chain based on PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_genai_utils.rag.pdf import PDFRetrievalChain\n",
    "\n",
    "pdf_path = \"../../../sample-docs/AutoGen-paper.pdf\"\n",
    "\n",
    "pdf = PDFRetrievalChain(\n",
    "    source_uri=[pdf_path],\n",
    "    loader_type=\"PDFPlumber\",\n",
    "    model_name=azure_openai_chat_deployment_name,\n",
    "    embedding_name=azure_openai_embedding_deployment_name,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ").create_chain()\n",
    "\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain\n",
    "\n",
    "question = \"What is AutoGen's main features?\"\n",
    "docs = pdf_retriever.invoke(question)\n",
    "\n",
    "# Non-streaming\n",
    "# results = pdf_chain.invoke({\"chat_history\": \"\", \"question\": question, \"context\": docs})\n",
    "\n",
    "# Streaming\n",
    "for text in pdf_chain.stream(\n",
    "    {\"chat_history\": \"\", \"question\": question, \"context\": docs}\n",
    "):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d177012",
   "metadata": {},
   "source": [
    "### Define your LLM\n",
    "\n",
    "This hands-on only uses the `gpt-4o-mini`, but you can utilize multiple models in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(model=azure_openai_chat_deployment_name, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47f9ae",
   "metadata": {},
   "source": [
    "### Question-Retrieval Grader\n",
    "\n",
    "Construct a retrieval grader that evaluates the relevance of the retrieved documents to the input question. The retrieval grader should take the input question and the retrieved documents as input and output a relevance score for each document.<br>\n",
    "Note that the retrieval grader should be able to handle **multiple documents** as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7932d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"A binary score to determine the relevance of the retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b7b76",
   "metadata": {},
   "source": [
    "Test the retrieval grader. For testing, we only show the result of the a single document, not the entire document set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39048bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is AutoGen's main features?\"\n",
    "docs = pdf_retriever.invoke(question)\n",
    "\n",
    "# Extract the page content of the second document retrieved\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ae75e",
   "metadata": {},
   "source": [
    "### Answer Generator\n",
    "\n",
    "Construct a LLM Generation node. This is a Naive RAG chain that generates an answer based on the retrieved documents. \n",
    "\n",
    "We recommend you to use more advanced RAG chain for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "if has_langchain_key:\n",
    "    print(f\"Load prompt from LangChain Hub.\")\n",
    "    prompt = hub.pull(\"daekeun-ml/rag-baseline\")\n",
    "else:\n",
    "    print(\"LANGCHAIN_API_KEY is not set. Load prompt from YAML file.\")\n",
    "    prompt = load_prompt(\"prompts/rag-baseline.yaml\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f59f7",
   "metadata": {},
   "source": [
    "### Question Re-writer\n",
    "\n",
    "Construct a `question_rewriter` node to rewrite the question for web search optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eabae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \n",
    "for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1acc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[Original question] {question}\")\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a72cc",
   "metadata": {},
   "source": [
    "### Web Search Tool\n",
    "\n",
    "Web search tool is used to enhance the context. <br>\n",
    "\n",
    "It is used when all the documents do not meet the relevance threshold or the evaluator is not confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_genai_utils.tools import BingSearch\n",
    "\n",
    "WEB_SEARCH_FORMAT_OUTPUT = True\n",
    "\n",
    "web_search_tool = BingSearch(\n",
    "    max_results=3,\n",
    "    locale=\"en-US\",\n",
    "    include_news=True,\n",
    "    include_entity=False,\n",
    "    format_output=WEB_SEARCH_FORMAT_OUTPUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = web_search_tool.invoke({\"query\": question})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30cb22",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ðŸ§ª Step 2. Define the Graph\n",
    "---\n",
    "\n",
    "### State Definition\n",
    "\n",
    "- `question`: Question from the user\n",
    "- `generation`: Generated answer\n",
    "- `web_search`: Whether to use web search or not\n",
    "- `documents`: Retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07cb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: Annotated[str, \"Question\"]\n",
    "    generation: Annotated[str, \"LLM Generation\"]\n",
    "    web_search: Annotated[str, \"Whether to add search\"]\n",
    "    documents: Annotated[List[str], \"Retrieved Documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bdd4b",
   "metadata": {},
   "source": [
    "### Define Nodes\n",
    "\n",
    "We will define the following nodes in the graph:\n",
    "\n",
    "- `retrieve`: Retrieve documents based on the user question.\n",
    "- `grade_documents`: Generate an answer based on the retrieved documents and user question.\n",
    "- `generate`: Grade documents based on their relevance to the user question.\n",
    "- `rewrite_query`: Rewrite the user question to improve retrieval performance.\n",
    "- `web_search`: Search the web for additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d7f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the user question.\n",
    "    \"\"\"\n",
    "    print(\"==== [RETRIEVE] ====\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"Generate an answer based on the retrieved documents and user question.\"\"\"\n",
    "    print(\"==== [GENERATE] ====\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state: GraphState):\n",
    "    \"\"\"Grade documents based on their relevance to the user question.\"\"\"\n",
    "    print(\"\\n==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    relevant_doc_count = 0\n",
    "\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "\n",
    "        if grade == \"yes\":\n",
    "            print(\"==== [GRADE: DOCUMENT RELEVANT] ====\")\n",
    "            # Add related documents to filtered_docs\n",
    "            filtered_docs.append(d)\n",
    "            relevant_doc_count += 1\n",
    "        else:\n",
    "            print(\"==== [GRADE: DOCUMENT NOT RELEVANT] ====\")\n",
    "            continue\n",
    "\n",
    "    # Web search if no relevant documents\n",
    "    web_search = \"Yes\" if relevant_doc_count == 0 else \"No\"\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def rewrite_query(state: GraphState):\n",
    "    \"\"\"Rewrite the user question to improve web search results\"\"\"\n",
    "    print(\"\\n==== [REWRITE QUERY] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state: GraphState):\n",
    "    \"\"\"Search the web for additional information.\"\"\"\n",
    "    print(\"\\n==== [WEB SEARCH] ====\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    # Convert search results to document format\n",
    "    if WEB_SEARCH_FORMAT_OUTPUT:\n",
    "        web_results = \"\\n\".join(docs)\n",
    "    else:\n",
    "        web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52091fa6",
   "metadata": {},
   "source": [
    "### Define Conditional Nodes\n",
    "\n",
    "- `decide_to_generate`: Decide whether to generate an answer based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Access the graded documents and decide whether to generate an answer or rewrite the query.\n",
    "    \"\"\"\n",
    "    print(\"==== [ASSESS GRADED DOCUMENTS] ====\")\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # If all documents are not relevant to the question, rewrite the query and search the web\n",
    "        print(\n",
    "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY] ====\"\n",
    "        )\n",
    "        # Route to the query rewrite node\n",
    "        return \"rewrite_query\"\n",
    "    else:\n",
    "        # Generate an answer based on the retrieved documents\n",
    "        print(\"==== [DECISION: GENERATE] ====\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadaa1e",
   "metadata": {},
   "source": [
    "### Construct the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99977214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Node definition\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"rewrite_query\", rewrite_query)\n",
    "workflow.add_node(\"web_search_node\", web_search)\n",
    "\n",
    "# Edge connections\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"rewrite_query\": \"rewrite_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"rewrite_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98c318",
   "metadata": {},
   "source": [
    "### Visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure_genai_utils.graphs import visualize_langgraph\n",
    "\n",
    "visualize_langgraph(app, xray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357c3d7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ðŸ§ª Step 3. Execute the Graph\n",
    "---\n",
    "\n",
    "### Execute the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a06ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from azure_genai_utils.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "inputs = {\n",
    "    \"question\": \"What is AutoGen's main features?\",\n",
    "}\n",
    "\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"rewrite_query\", \"web_search_node\", \"generate\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28667d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "\n",
    "inputs = {\n",
    "    \"question\": \"Please tell me the Microsoft brief history.\",\n",
    "}\n",
    "stream_graph(\n",
    "    app,\n",
    "    inputs,\n",
    "    config,\n",
    "    [\"retrieve\", \"grade_documents\", \"rewrite_query\", \"web_search_node\", \"generate\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
